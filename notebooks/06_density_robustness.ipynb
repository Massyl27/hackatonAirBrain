{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06 -- Density Robustness Test (v7.3)\n",
    "\n",
    "**Airbus evaluation criterion #3: Robustness** -- maintain detection performance at reduced point cloud densities.\n",
    "\n",
    "This notebook sub-samples point clouds at **100%, 75%, 50%, 25%** density and runs the full inference + clustering pipeline on each level, comparing box counts and class distributions to measure degradation.\n",
    "\n",
    "Designed for **Google Colab** (GPU runtime recommended for speed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "!pip install -q h5py scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import time\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# --- Paths ---\n",
    "DRIVE_BASE = \"/content/drive/MyDrive/airbus_hackathon\"\n",
    "INPUT_DIR = f\"{DRIVE_BASE}/data\"\n",
    "OUTPUT_DIR = f\"{DRIVE_BASE}/outputs/density_test_v73\"\n",
    "CKPT_V5 = f\"{DRIVE_BASE}/checkpoints_v5/best_model_v5.pt\"\n",
    "CKPT_V4 = f\"{DRIVE_BASE}/checkpoints_v4/best_model_v4.pt\"\n",
    "CKPT_PATH = CKPT_V5 if os.path.exists(CKPT_V5) else CKPT_V4\n",
    "SINGLE_SCENE = \"scene_8\"\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"Checkpoint: {CKPT_PATH}\")\n",
    "print(f\"Input dir:  {INPUT_DIR}\")\n",
    "print(f\"Output dir: {OUTPUT_DIR}\")\n",
    "print(f\"Scene:      {SINGLE_SCENE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config (v7.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 5\n",
    "IN_CHANNELS = 5\n",
    "CHUNK_SIZE = 65536\n",
    "\n",
    "CLASS_NAMES = {0: \"background\", 1: \"antenna\", 2: \"cable\", 3: \"electric_pole\", 4: \"wind_turbine\"}\n",
    "CLASS_LABELS_CSV = {1: \"Antenna\", 2: \"Cable\", 3: \"Electric Pole\", 4: \"Wind Turbine\"}\n",
    "\n",
    "DBSCAN_PARAMS = {\n",
    "    1: {\"eps\": 2.0, \"min_samples\": 15},\n",
    "    2: {\"eps\": 5.0, \"min_samples\": 5},\n",
    "    3: {\"eps\": 2.0, \"min_samples\": 8},\n",
    "    4: {\"eps\": 5.0, \"min_samples\": 20},\n",
    "}\n",
    "\n",
    "CABLE_MERGE_ANGLE_DEG = 15.0\n",
    "CABLE_MERGE_GAP_M = 10.0\n",
    "\n",
    "CONFIDENCE_THRESHOLD_PER_CLASS = {\n",
    "    1: 0.40,\n",
    "    2: 0.27,\n",
    "    3: 0.25,\n",
    "    4: 0.30,\n",
    "}\n",
    "CONFIDENCE_THRESHOLD_DEFAULT = 0.3\n",
    "\n",
    "BOX_CONFIDENCE_THRESHOLD_PER_CLASS = {\n",
    "    1: 0.70,\n",
    "    2: 0.55,\n",
    "    3: 0.45,\n",
    "    4: 0.60,\n",
    "}\n",
    "BOX_CONFIDENCE_THRESHOLD_DEFAULT = 0.6\n",
    "\n",
    "MIN_POINTS_PER_BOX = {1: 15, 2: 3, 3: 5, 4: 15}\n",
    "MAX_DIM_PER_CLASS = {1: 200.0, 2: 400.0, 3: 100.0, 4: 250.0}\n",
    "NMS_IOU_THRESHOLD = 0.3\n",
    "\n",
    "print(\"Config v7.3 loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedMLP(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, bn=True):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(in_ch, out_ch, 1, bias=not bn)\n",
    "        self.bn = nn.BatchNorm1d(out_ch) if bn else None\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        if self.bn:\n",
    "            x = self.bn(x)\n",
    "        return F.relu(x, inplace=True)\n",
    "\n",
    "\n",
    "class PointNetSegV4(nn.Module):\n",
    "    def __init__(self, in_channels=5, num_classes=5):\n",
    "        super().__init__()\n",
    "        self.enc1 = SharedMLP(in_channels, 64)\n",
    "        self.enc2 = SharedMLP(64, 128)\n",
    "        self.enc3 = SharedMLP(128, 256)\n",
    "        self.enc4 = SharedMLP(256, 512)\n",
    "        self.enc5 = SharedMLP(512, 1024)\n",
    "        self.seg1 = SharedMLP(64 + 128 + 256 + 512 + 1024, 512)\n",
    "        self.seg2 = SharedMLP(512, 256)\n",
    "        self.seg3 = SharedMLP(256, 128)\n",
    "        self.dropout1 = nn.Dropout(0.4)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.head = nn.Conv1d(128, num_classes, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, _ = x.shape\n",
    "        x = x.transpose(1, 2)\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(e1)\n",
    "        e3 = self.enc3(e2)\n",
    "        e4 = self.enc4(e3)\n",
    "        e5 = self.enc5(e4)\n",
    "        g = e5.max(dim=2, keepdim=True)[0].expand(-1, -1, N)\n",
    "        seg = torch.cat([e1, e2, e3, e4, g], dim=1)\n",
    "        seg = self.seg1(seg)\n",
    "        seg = self.dropout1(seg)\n",
    "        seg = self.seg2(seg)\n",
    "        seg = self.dropout2(seg)\n",
    "        seg = self.seg3(seg)\n",
    "        seg = self.head(seg)\n",
    "        return seg.transpose(1, 2)\n",
    "\n",
    "\n",
    "# --- Load checkpoint ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "model = PointNetSegV4(in_channels=IN_CHANNELS, num_classes=NUM_CLASSES).to(device)\n",
    "ckpt = torch.load(CKPT_PATH, map_location=device, weights_only=False)\n",
    "model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "model.eval()\n",
    "\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model loaded from {CKPT_PATH}\")\n",
    "print(f\"Parameters: {n_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDF5 Reader + Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frame_boundaries(h5_path, dataset_name=\"lidar_points\", chunk_size=2_000_000):\n",
    "    change_indices = []\n",
    "    with h5py.File(h5_path, \"r\") as f:\n",
    "        ds = f[dataset_name]\n",
    "        n = ds.shape[0]\n",
    "        prev_last_pose = None\n",
    "        for offset in range(0, n, chunk_size):\n",
    "            end = min(offset + chunk_size, n)\n",
    "            chunk = ds[offset:end]\n",
    "            ex, ey, ez, eyaw = chunk[\"ego_x\"], chunk[\"ego_y\"], chunk[\"ego_z\"], chunk[\"ego_yaw\"]\n",
    "            if prev_last_pose is not None:\n",
    "                cur_first = (int(ex[0]), int(ey[0]), int(ez[0]), int(eyaw[0]))\n",
    "                if cur_first != prev_last_pose:\n",
    "                    change_indices.append(offset)\n",
    "            changes = np.where(\n",
    "                (np.diff(ex) != 0) | (np.diff(ey) != 0) |\n",
    "                (np.diff(ez) != 0) | (np.diff(eyaw) != 0)\n",
    "            )[0] + 1\n",
    "            for c in changes:\n",
    "                change_indices.append(offset + int(c))\n",
    "            prev_last_pose = (int(ex[-1]), int(ey[-1]), int(ez[-1]), int(eyaw[-1]))\n",
    "            del chunk, ex, ey, ez, eyaw\n",
    "            gc.collect()\n",
    "    starts = [0] + change_indices\n",
    "    ends = change_indices + [n]\n",
    "    frames = []\n",
    "    with h5py.File(h5_path, \"r\") as f:\n",
    "        ds = f[dataset_name]\n",
    "        for s, e in zip(starts, ends):\n",
    "            row = ds[s]\n",
    "            frames.append((s, e, int(row[\"ego_x\"]), int(row[\"ego_y\"]),\n",
    "                           int(row[\"ego_z\"]), int(row[\"ego_yaw\"])))\n",
    "    return frames\n",
    "\n",
    "\n",
    "def read_frame_for_inference(h5_path, start, end, dataset_name=\"lidar_points\"):\n",
    "    with h5py.File(h5_path, \"r\") as f:\n",
    "        chunk = f[dataset_name][start:end]\n",
    "    valid = chunk[chunk[\"distance_cm\"] > 0]\n",
    "    del chunk\n",
    "    dist_m = valid[\"distance_cm\"].astype(np.float64) / 100.0\n",
    "    az_rad = np.radians(valid[\"azimuth_raw\"].astype(np.float64) / 100.0)\n",
    "    el_rad = np.radians(valid[\"elevation_raw\"].astype(np.float64) / 100.0)\n",
    "    cos_el = np.cos(el_rad)\n",
    "    x = dist_m * cos_el * np.cos(az_rad)\n",
    "    y = -dist_m * cos_el * np.sin(az_rad)\n",
    "    z = dist_m * np.sin(el_rad)\n",
    "    xyz = np.column_stack((x, y, z)).astype(np.float32)\n",
    "    refl_norm = (valid[\"reflectivity\"].astype(np.float32) / 255.0).reshape(-1, 1)\n",
    "    dist_norm = (dist_m.astype(np.float32) / 300.0).reshape(-1, 1)\n",
    "    features = np.concatenate([xyz, refl_norm, dist_norm], axis=1)\n",
    "    del valid, dist_m, az_rad, el_rad, cos_el, x, y, z\n",
    "    return xyz, features\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_frame(model, features_np, device, chunk_size=CHUNK_SIZE):\n",
    "    n = len(features_np)\n",
    "    predictions = np.zeros(n, dtype=np.int64)\n",
    "    confidences = np.zeros(n, dtype=np.float32)\n",
    "    for start in range(0, n, chunk_size):\n",
    "        end = min(start + chunk_size, n)\n",
    "        chunk = features_np[start:end]\n",
    "        pad_to = max(len(chunk), 128)\n",
    "        if len(chunk) < pad_to:\n",
    "            padded = np.zeros((pad_to, chunk.shape[1]), dtype=np.float32)\n",
    "            padded[:len(chunk)] = chunk\n",
    "        else:\n",
    "            padded = chunk\n",
    "        tensor = torch.from_numpy(padded).unsqueeze(0).to(device)\n",
    "        logits = model(tensor)\n",
    "        probs = F.softmax(logits[0, :len(chunk)], dim=-1)\n",
    "        conf, preds = probs.max(dim=-1)\n",
    "        preds = preds.cpu().numpy()\n",
    "        conf = conf.cpu().numpy()\n",
    "        # Per-class confidence threshold\n",
    "        for cid in range(1, 5):\n",
    "            thresh = CONFIDENCE_THRESHOLD_PER_CLASS.get(cid, CONFIDENCE_THRESHOLD_DEFAULT)\n",
    "            low_conf = (preds == cid) & (conf < thresh)\n",
    "            preds[low_conf] = 0\n",
    "        predictions[start:end] = preds\n",
    "        confidences[start:end] = conf\n",
    "        del tensor, logits, probs, preds, conf\n",
    "    return predictions, confidences\n",
    "\n",
    "\n",
    "print(\"HDF5 reader + inference functions loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering + Post-processing (v7.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_oriented_bbox(points_m):\n",
    "    center_xyz = points_m.mean(axis=0)\n",
    "    centered = points_m - center_xyz\n",
    "    cov = np.cov(centered.T)\n",
    "    if np.any(np.isnan(cov)) or np.any(np.isinf(cov)):\n",
    "        mins, maxs = points_m.min(axis=0), points_m.max(axis=0)\n",
    "        return {\"center_xyz\": (mins + maxs) / 2.0, \"dimensions\": maxs - mins, \"yaw\": 0.0}\n",
    "    try:\n",
    "        eigenvalues, eigenvectors = np.linalg.eigh(cov)\n",
    "    except np.linalg.LinAlgError:\n",
    "        mins, maxs = points_m.min(axis=0), points_m.max(axis=0)\n",
    "        return {\"center_xyz\": (mins + maxs) / 2.0, \"dimensions\": maxs - mins, \"yaw\": 0.0}\n",
    "    order = eigenvalues.argsort()[::-1]\n",
    "    eigenvectors = eigenvectors[:, order]\n",
    "    projected = centered @ eigenvectors\n",
    "    mins, maxs = projected.min(axis=0), projected.max(axis=0)\n",
    "    dimensions = maxs - mins\n",
    "    box_center_pca = (mins + maxs) / 2.0\n",
    "    center_xyz = center_xyz + eigenvectors @ box_center_pca\n",
    "    axis1_xy = eigenvectors[:2, 0]\n",
    "    yaw = np.arctan2(axis1_xy[1], axis1_xy[0])\n",
    "    return {\"center_xyz\": center_xyz, \"dimensions\": dimensions, \"yaw\": float(yaw)}\n",
    "\n",
    "\n",
    "def cluster_class_points(points_m, class_id, max_points=10000):\n",
    "    params = DBSCAN_PARAMS[class_id]\n",
    "    eps, min_samples = params[\"eps\"], params[\"min_samples\"]\n",
    "    if len(points_m) < min_samples:\n",
    "        return []\n",
    "    full_points = points_m\n",
    "    if len(points_m) > max_points:\n",
    "        idx = np.random.choice(len(points_m), max_points, replace=False)\n",
    "        points_m = points_m[idx]\n",
    "    labels = DBSCAN(eps=eps, min_samples=min_samples, algorithm=\"ball_tree\").fit_predict(points_m)\n",
    "    if len(full_points) > max_points:\n",
    "        from sklearn.neighbors import BallTree\n",
    "        sampled_mask = labels >= 0\n",
    "        if sampled_mask.sum() == 0:\n",
    "            return []\n",
    "        tree = BallTree(points_m[sampled_mask])\n",
    "        _, indices = tree.query(full_points, k=1)\n",
    "        full_labels = labels[sampled_mask][indices.ravel()]\n",
    "        dists = np.linalg.norm(full_points - points_m[sampled_mask][indices.ravel()], axis=1)\n",
    "        full_labels[dists > eps * 2] = -1\n",
    "        labels = full_labels\n",
    "        points_m = full_points\n",
    "    clusters = []\n",
    "    for lbl in sorted(set(labels) - {-1}):\n",
    "        clusters.append(points_m[labels == lbl])\n",
    "    return clusters\n",
    "\n",
    "\n",
    "def merge_cable_clusters(clusters):\n",
    "    if len(clusters) <= 1:\n",
    "        return clusters\n",
    "    angle_thresh = np.radians(CABLE_MERGE_ANGLE_DEG)\n",
    "    gap_thresh = CABLE_MERGE_GAP_M\n",
    "    infos = []\n",
    "    for pts in clusters:\n",
    "        if len(pts) < 4:\n",
    "            infos.append({\"points\": pts, \"center\": pts.mean(axis=0), \"axis1\": None})\n",
    "            continue\n",
    "        centered = pts - pts.mean(axis=0)\n",
    "        cov = np.cov(centered.T)\n",
    "        if np.any(np.isnan(cov)) or np.any(np.isinf(cov)):\n",
    "            infos.append({\"points\": pts, \"center\": pts.mean(axis=0), \"axis1\": None})\n",
    "            continue\n",
    "        try:\n",
    "            eigvals, eigvecs = np.linalg.eigh(cov)\n",
    "        except np.linalg.LinAlgError:\n",
    "            infos.append({\"points\": pts, \"center\": pts.mean(axis=0), \"axis1\": None})\n",
    "            continue\n",
    "        axis1 = eigvecs[:, eigvals.argsort()[::-1][0]]\n",
    "        if axis1[0] < 0:\n",
    "            axis1 = -axis1\n",
    "        infos.append({\"points\": pts, \"center\": pts.mean(axis=0), \"axis1\": axis1})\n",
    "    merged_flags = [False] * len(infos)\n",
    "    result = []\n",
    "    for i in range(len(infos)):\n",
    "        if merged_flags[i]:\n",
    "            continue\n",
    "        current = infos[i][\"points\"]\n",
    "        if infos[i][\"axis1\"] is not None:\n",
    "            for j in range(i + 1, len(infos)):\n",
    "                if merged_flags[j] or infos[j][\"axis1\"] is None:\n",
    "                    continue\n",
    "                dot = min(abs(np.dot(infos[i][\"axis1\"], infos[j][\"axis1\"])), 1.0)\n",
    "                if np.arccos(dot) > angle_thresh:\n",
    "                    continue\n",
    "                cdist = np.linalg.norm(infos[i][\"center\"] - infos[j][\"center\"])\n",
    "                ext_i = np.abs((infos[i][\"points\"] - infos[i][\"center\"]) @ infos[i][\"axis1\"]).max()\n",
    "                ext_j = np.abs((infos[j][\"points\"] - infos[j][\"center\"]) @ infos[j][\"axis1\"]).max()\n",
    "                if cdist - ext_i - ext_j <= gap_thresh:\n",
    "                    current = np.vstack([current, infos[j][\"points\"]])\n",
    "                    merged_flags[j] = True\n",
    "        result.append(current)\n",
    "    return result\n",
    "\n",
    "\n",
    "def filter_boxes(boxes):\n",
    "    filtered = []\n",
    "    for box in boxes:\n",
    "        cid = box[\"class_id\"]\n",
    "        if box[\"num_points\"] < MIN_POINTS_PER_BOX.get(cid, 3):\n",
    "            continue\n",
    "        if max(box[\"dimensions\"]) > MAX_DIM_PER_CLASS.get(cid, 500.0):\n",
    "            continue\n",
    "        filtered.append(box)\n",
    "    return filtered\n",
    "\n",
    "\n",
    "def _box_iou_3d(box_a, box_b):\n",
    "    ca, da = box_a[\"center_xyz\"], box_a[\"dimensions\"]\n",
    "    cb, db = box_b[\"center_xyz\"], box_b[\"dimensions\"]\n",
    "    ha, hb = da / 2.0, db / 2.0\n",
    "    overlap = np.maximum(0, np.minimum(ca + ha, cb + hb) - np.maximum(ca - ha, cb - hb))\n",
    "    inter = overlap[0] * overlap[1] * overlap[2]\n",
    "    vol_a, vol_b = da[0] * da[1] * da[2], db[0] * db[1] * db[2]\n",
    "    union = vol_a + vol_b - inter\n",
    "    return inter / union if union > 0 else 0.0\n",
    "\n",
    "\n",
    "def nms_boxes(boxes, iou_threshold=NMS_IOU_THRESHOLD):\n",
    "    if len(boxes) <= 1:\n",
    "        return boxes\n",
    "    by_class = {}\n",
    "    for box in boxes:\n",
    "        by_class.setdefault(box[\"class_id\"], []).append(box)\n",
    "    result = []\n",
    "    for cid, class_boxes in by_class.items():\n",
    "        class_boxes.sort(key=lambda b: b[\"num_points\"], reverse=True)\n",
    "        suppressed = [False] * len(class_boxes)\n",
    "        for i in range(len(class_boxes)):\n",
    "            if suppressed[i]:\n",
    "                continue\n",
    "            result.append(class_boxes[i])\n",
    "            for j in range(i + 1, len(class_boxes)):\n",
    "                if not suppressed[j] and _box_iou_3d(class_boxes[i], class_boxes[j]) > iou_threshold:\n",
    "                    suppressed[j] = True\n",
    "    return result\n",
    "\n",
    "\n",
    "def reclassify_by_geometry(boxes):\n",
    "    \"\"\"Reclassify boxes based on geometric properties.\"\"\"\n",
    "    for box in boxes:\n",
    "        if box[\"class_id\"] != 1:\n",
    "            continue\n",
    "        dims = box[\"dimensions\"]\n",
    "        sorted_dims = sorted(dims, reverse=True)\n",
    "        longest, middle, shortest = sorted_dims\n",
    "        if middle > 0 and longest / middle > 5.0 and shortest < 1.0:\n",
    "            box[\"class_id\"] = 2\n",
    "            box[\"class_label\"] = CLASS_LABELS_CSV[2]\n",
    "        elif longest > 15.0 and box[\"num_points\"] > 200:\n",
    "            box[\"class_id\"] = 4\n",
    "            box[\"class_label\"] = CLASS_LABELS_CSV[4]\n",
    "    return boxes\n",
    "\n",
    "\n",
    "def predictions_to_boxes(xyz_m, predictions, confidences=None):\n",
    "    boxes = []\n",
    "    for cid in range(1, 5):\n",
    "        mask = predictions == cid\n",
    "        if mask.sum() == 0:\n",
    "            continue\n",
    "        class_points = xyz_m[mask]\n",
    "        class_conf = confidences[mask] if confidences is not None else None\n",
    "        clusters = cluster_class_points(class_points, cid)\n",
    "        if cid == 2 and len(clusters) > 1:\n",
    "            clusters = merge_cable_clusters(clusters)\n",
    "        for pts in clusters:\n",
    "            if len(pts) < 3:\n",
    "                continue\n",
    "            box_confidence = 0.0\n",
    "            if class_conf is not None:\n",
    "                from sklearn.neighbors import BallTree\n",
    "                tree = BallTree(class_points)\n",
    "                _, indices = tree.query(pts, k=1)\n",
    "                box_confidence = float(class_conf[indices.ravel()].mean())\n",
    "            bbox = pca_oriented_bbox(pts)\n",
    "            boxes.append({\n",
    "                \"center_xyz\": bbox[\"center_xyz\"],\n",
    "                \"dimensions\": bbox[\"dimensions\"],\n",
    "                \"yaw\": bbox[\"yaw\"],\n",
    "                \"class_id\": cid,\n",
    "                \"class_label\": CLASS_LABELS_CSV[cid],\n",
    "                \"num_points\": len(pts),\n",
    "                \"confidence\": box_confidence,\n",
    "            })\n",
    "    # Geometric reclassification\n",
    "    boxes = reclassify_by_geometry(boxes)\n",
    "    # Per-class box confidence filter\n",
    "    filtered = []\n",
    "    for b in boxes:\n",
    "        thresh = BOX_CONFIDENCE_THRESHOLD_PER_CLASS.get(\n",
    "            b[\"class_id\"], BOX_CONFIDENCE_THRESHOLD_DEFAULT)\n",
    "        if b[\"confidence\"] >= thresh:\n",
    "            filtered.append(b)\n",
    "    boxes = filtered\n",
    "    boxes = filter_boxes(boxes)\n",
    "    boxes = nms_boxes(boxes)\n",
    "    return boxes\n",
    "\n",
    "\n",
    "print(\"Clustering + post-processing functions loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Density sub-sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsample_features(xyz_m, features, density_pct):\n",
    "    \"\"\"Randomly subsample points to simulate reduced density.\"\"\"\n",
    "    if density_pct >= 100:\n",
    "        return xyz_m, features\n",
    "    n = len(xyz_m)\n",
    "    k = max(1, int(n * density_pct / 100.0))\n",
    "    idx = np.random.choice(n, k, replace=False)\n",
    "    idx.sort()\n",
    "    return xyz_m[idx], features[idx]\n",
    "\n",
    "\n",
    "print(\"Subsample function loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run density test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5_path = os.path.join(INPUT_DIR, f\"{SINGLE_SCENE}.h5\")\n",
    "print(f\"Loading frame boundaries from {h5_path}...\")\n",
    "frames_info = get_frame_boundaries(h5_path)\n",
    "n_frames = len(frames_info)\n",
    "print(f\"Found {n_frames} frames in {SINGLE_SCENE}\")\n",
    "\n",
    "densities = [100, 75, 50, 25]\n",
    "results = {\n",
    "    d: {\n",
    "        \"total_boxes\": 0,\n",
    "        \"class_counts\": {1: 0, 2: 0, 3: 0, 4: 0},\n",
    "        \"frames_with_boxes\": 0,\n",
    "        \"total_points\": 0,\n",
    "    }\n",
    "    for d in densities\n",
    "}\n",
    "\n",
    "t_start = time.time()\n",
    "\n",
    "for idx in range(n_frames):\n",
    "    start, end, ego_x, ego_y, ego_z, ego_yaw = frames_info[idx]\n",
    "    xyz_m, features = read_frame_for_inference(h5_path, start, end)\n",
    "    if len(xyz_m) == 0:\n",
    "        continue\n",
    "\n",
    "    for density in densities:\n",
    "        np.random.seed(42 + idx)  # Reproducible per frame\n",
    "        xyz_sub, feat_sub = subsample_features(xyz_m, features, density)\n",
    "\n",
    "        predictions, confidences = predict_frame(model, feat_sub, device)\n",
    "        boxes = predictions_to_boxes(xyz_sub, predictions, confidences)\n",
    "\n",
    "        r = results[density]\n",
    "        r[\"total_boxes\"] += len(boxes)\n",
    "        r[\"total_points\"] += len(xyz_sub)\n",
    "        if boxes:\n",
    "            r[\"frames_with_boxes\"] += 1\n",
    "        for box in boxes:\n",
    "            r[\"class_counts\"][box[\"class_id\"]] += 1\n",
    "\n",
    "        del xyz_sub, feat_sub, predictions, confidences, boxes\n",
    "\n",
    "    del xyz_m, features\n",
    "    gc.collect()\n",
    "\n",
    "    if (idx + 1) % 10 == 0 or idx == n_frames - 1:\n",
    "        elapsed = time.time() - t_start\n",
    "        print(f\"  {idx+1}/{n_frames} frames ({elapsed:.0f}s)\")\n",
    "\n",
    "total_elapsed = time.time() - t_start\n",
    "print(f\"\\nDone! {n_frames} frames processed in {total_elapsed:.0f}s\")\n",
    "\n",
    "# --- Print results table ---\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"DENSITY ROBUSTNESS TEST -- {SINGLE_SCENE} -- {n_frames} frames ({total_elapsed:.0f}s)\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "header = f\"{'Density':>10} | {'Boxes':>8} | {'Boxes/fr':>8} | {'FrWithBox':>9} | {'Antenna':>8} | {'Cable':>8} | {'ElecPole':>8} | {'WindTurb':>8}\"\n",
    "print(header)\n",
    "print(\"-\" * len(header))\n",
    "\n",
    "report_lines = [header, \"-\" * len(header)]\n",
    "ref_boxes = None\n",
    "\n",
    "for density in densities:\n",
    "    r = results[density]\n",
    "    avg = r[\"total_boxes\"] / max(n_frames, 1)\n",
    "    cc = r[\"class_counts\"]\n",
    "    line = (\n",
    "        f\"{density:>9}% | {r['total_boxes']:>8} | {avg:>8.1f} | {r['frames_with_boxes']:>9} | \"\n",
    "        f\"{cc[1]:>8} | {cc[2]:>8} | {cc[3]:>8} | {cc[4]:>8}\"\n",
    "    )\n",
    "    print(line)\n",
    "    report_lines.append(line)\n",
    "    if density == 100:\n",
    "        ref_boxes = r[\"total_boxes\"]\n",
    "\n",
    "print(f\"\\nRetention rates vs 100% density:\")\n",
    "report_lines.append(f\"\\nRetention rates vs 100% density:\")\n",
    "for density in densities:\n",
    "    r = results[density]\n",
    "    rate = r[\"total_boxes\"] / max(ref_boxes, 1) * 100\n",
    "    line = f\"  {density}% density -> {rate:.1f}% of boxes retained\"\n",
    "    print(line)\n",
    "    report_lines.append(line)\n",
    "\n",
    "# --- Save report ---\n",
    "report_path = os.path.join(OUTPUT_DIR, f\"density_report_{SINGLE_SCENE}.txt\")\n",
    "with open(report_path, \"w\") as f:\n",
    "    f.write(f\"Density Robustness Test (v7.3) -- {SINGLE_SCENE}\\n\")\n",
    "    f.write(f\"Frames: {n_frames}, Time: {total_elapsed:.0f}s\\n\\n\")\n",
    "    for line in report_lines:\n",
    "        f.write(line + \"\\n\")\n",
    "print(f\"\\nReport saved: {report_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Summary DataFrame ---\n",
    "rows = []\n",
    "ref_total = results[100][\"total_boxes\"]\n",
    "ref_cc = results[100][\"class_counts\"]\n",
    "\n",
    "for density in densities:\n",
    "    r = results[density]\n",
    "    cc = r[\"class_counts\"]\n",
    "    total_retention = r[\"total_boxes\"] / max(ref_total, 1) * 100\n",
    "    rows.append({\n",
    "        \"Density (%)\": density,\n",
    "        \"Total Boxes\": r[\"total_boxes\"],\n",
    "        \"Boxes/Frame\": round(r[\"total_boxes\"] / max(n_frames, 1), 1),\n",
    "        \"Frames w/ Boxes\": r[\"frames_with_boxes\"],\n",
    "        \"Antenna\": cc[1],\n",
    "        \"Cable\": cc[2],\n",
    "        \"Elec. Pole\": cc[3],\n",
    "        \"Wind Turb.\": cc[4],\n",
    "        \"Retention (%)\": round(total_retention, 1),\n",
    "        \"Ant. Ret. (%)\": round(cc[1] / max(ref_cc[1], 1) * 100, 1),\n",
    "        \"Cab. Ret. (%)\": round(cc[2] / max(ref_cc[2], 1) * 100, 1),\n",
    "        \"Pole Ret. (%)\": round(cc[3] / max(ref_cc[3], 1) * 100, 1),\n",
    "        \"WT Ret. (%)\": round(cc[4] / max(ref_cc[4], 1) * 100, 1),\n",
    "    })\n",
    "\n",
    "df_summary = pd.DataFrame(rows)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DENSITY ROBUSTNESS SUMMARY (v7.3)\")\n",
    "print(\"=\" * 80)\n",
    "print(df_summary.to_string(index=False))\n",
    "\n",
    "# Save CSV\n",
    "csv_path = os.path.join(OUTPUT_DIR, f\"density_summary_{SINGLE_SCENE}.csv\")\n",
    "df_summary.to_csv(csv_path, index=False)\n",
    "print(f\"\\nSummary saved: {csv_path}\")\n",
    "\n",
    "# Key insight\n",
    "ret_25 = results[25][\"total_boxes\"] / max(ref_total, 1) * 100\n",
    "print(f\"\\n--- Key metric: at 25% density, {ret_25:.1f}% of boxes retained ---\")\n",
    "if ret_25 >= 70:\n",
    "    print(\"GOOD: Model is robust to density reduction.\")\n",
    "elif ret_25 >= 50:\n",
    "    print(\"MODERATE: Some degradation at low density.\")\n",
    "else:\n",
    "    print(\"WARNING: Significant degradation at low density -- may need data augmentation.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}