{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 08 — mAP@IoU=0.5 Evaluation (v7.3)\n",
    "\n",
    "This notebook evaluates the **detection quality** of our model predictions against ground truth boxes on **scene_8** (validation set).\n",
    "\n",
    "**mAP@IoU=0.5** is Airbus evaluation criterion #1 — the single most important metric.\n",
    "\n",
    "- **GT boxes**: `outputs/gt_boxes_all.csv` (generated by `build_gt_boxes.py`, internal class_id 1-4)\n",
    "- **Predicted boxes**: `outputs/pred_v7/scene_8.csv` (Airbus format, class_ID 0-3)\n",
    "- **IoU**: Axis-aligned 3D IoU (ignoring yaw rotation — standard simplification)\n",
    "- **AP**: PASCAL VOC all-points interpolation per class, then mean across 4 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Imports OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Paths & config ──────────────────────────────────────────────────────────\n",
    "DRIVE_BASE = \"/content/drive/MyDrive/airbus_hackathon\"\n",
    "\n",
    "GT_CSV   = f\"{DRIVE_BASE}/outputs/gt_boxes_all.csv\"\n",
    "PRED_CSV = f\"{DRIVE_BASE}/outputs/pred_v7/scene_8.csv\"\n",
    "\n",
    "IOU_THRESHOLD = 0.5\n",
    "\n",
    "# Internal class IDs (1-based, matching GT CSV)\n",
    "CLASS_NAMES = {\n",
    "    1: \"Antenna\",\n",
    "    2: \"Cable\",\n",
    "    3: \"Electric Pole\",\n",
    "    4: \"Wind Turbine\",\n",
    "}\n",
    "\n",
    "VALIDATION_SCENE = \"scene_8\"\n",
    "\n",
    "print(f\"GT CSV:   {GT_CSV}\")\n",
    "print(f\"Pred CSV: {PRED_CSV}\")\n",
    "print(f\"IoU threshold: {IOU_THRESHOLD}\")\n",
    "print(f\"Validation scene: {VALIDATION_SCENE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and inspect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Load GT boxes ────────────────────────────────────────────────────────────\n",
    "# GT CSV columns: scene,frame_idx,ego_x,ego_y,ego_z,ego_yaw,class_id,class_label,\n",
    "#                  num_points,center_x,center_y,center_z,width,length,height,yaw\n",
    "# class_id: 1=Antenna, 2=Cable, 3=Electric Pole, 4=Wind Turbine\n",
    "\n",
    "df_gt = pd.read_csv(GT_CSV)\n",
    "print(f\"GT boxes total: {len(df_gt)} rows, columns: {list(df_gt.columns)}\")\n",
    "print(f\"\\nScenes in GT: {sorted(df_gt['scene'].unique())}\")\n",
    "print(f\"\\nGT class distribution (all scenes):\")\n",
    "print(df_gt['class_label'].value_counts())\n",
    "\n",
    "# Filter to validation scene only\n",
    "df_gt_scene8 = df_gt[df_gt['scene'] == VALIDATION_SCENE].copy()\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"GT boxes in {VALIDATION_SCENE}: {len(df_gt_scene8)}\")\n",
    "print(f\"Frames: {df_gt_scene8['frame_idx'].nunique()}\")\n",
    "print(f\"\\nGT class distribution ({VALIDATION_SCENE}):\")\n",
    "print(df_gt_scene8['class_label'].value_counts())\n",
    "print(f\"\\nGT head:\")\n",
    "df_gt_scene8.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Load prediction boxes ────────────────────────────────────────────────────\n",
    "# Pred CSV columns (Airbus format): ego_x,ego_y,ego_z,ego_yaw,\n",
    "#   bbox_center_x,bbox_center_y,bbox_center_z,bbox_width,bbox_length,bbox_height,\n",
    "#   bbox_yaw,class_ID,class_label\n",
    "# class_ID: 0=Antenna, 1=Cable, 2=Electric Pole, 3=Wind Turbine\n",
    "\n",
    "df_pred = pd.read_csv(PRED_CSV)\n",
    "print(f\"Pred boxes: {len(df_pred)} rows, columns: {list(df_pred.columns)}\")\n",
    "\n",
    "# Convert Airbus class_ID (0-3) to internal class_id (1-4)\n",
    "df_pred['class_id_internal'] = df_pred['class_ID'] + 1\n",
    "\n",
    "print(f\"\\nPred class distribution (Airbus ID → internal):\")\n",
    "for cid_airbus in sorted(df_pred['class_ID'].unique()):\n",
    "    cid_internal = cid_airbus + 1\n",
    "    count = (df_pred['class_ID'] == cid_airbus).sum()\n",
    "    label = CLASS_NAMES.get(cid_internal, '???')\n",
    "    print(f\"  Airbus {cid_airbus} → internal {cid_internal} ({label}): {count}\")\n",
    "\n",
    "print(f\"\\nPred head:\")\n",
    "df_pred.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IoU 3D computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou_3d_axis_aligned(box_a, box_b):\n",
    "    \"\"\"Compute 3D IoU between two boxes (axis-aligned approximation).\n",
    "    \n",
    "    Each box is a dict with:\n",
    "      - 'center': np.array([x, y, z])  in meters\n",
    "      - 'dims':   np.array([w, l, h])  in meters\n",
    "    \n",
    "    Ignores yaw rotation — this is a standard simplification that gives\n",
    "    a lower-bound on the true oriented IoU.\n",
    "    \"\"\"\n",
    "    ca, da = box_a['center'], box_a['dims']\n",
    "    cb, db = box_b['center'], box_b['dims']\n",
    "    \n",
    "    # Half-extents\n",
    "    ha = da / 2.0\n",
    "    hb = db / 2.0\n",
    "    \n",
    "    # Overlap per axis: min of upper bounds - max of lower bounds\n",
    "    overlap = np.maximum(0.0, np.minimum(ca + ha, cb + hb) - np.maximum(ca - ha, cb - hb))\n",
    "    intersection = overlap[0] * overlap[1] * overlap[2]\n",
    "    \n",
    "    vol_a = da[0] * da[1] * da[2]\n",
    "    vol_b = db[0] * db[1] * db[2]\n",
    "    union = vol_a + vol_b - intersection\n",
    "    \n",
    "    if union <= 0:\n",
    "        return 0.0\n",
    "    return intersection / union\n",
    "\n",
    "\n",
    "# ── Quick sanity check ────────────────────────────────────────────────────────\n",
    "# Identical boxes → IoU = 1.0\n",
    "b = {'center': np.array([1, 2, 3]), 'dims': np.array([2, 2, 2])}\n",
    "assert abs(iou_3d_axis_aligned(b, b) - 1.0) < 1e-9, \"Sanity check failed\"\n",
    "\n",
    "# Non-overlapping boxes → IoU = 0.0\n",
    "b1 = {'center': np.array([0, 0, 0]), 'dims': np.array([1, 1, 1])}\n",
    "b2 = {'center': np.array([10, 10, 10]), 'dims': np.array([1, 1, 1])}\n",
    "assert iou_3d_axis_aligned(b1, b2) == 0.0, \"Sanity check failed\"\n",
    "\n",
    "# Partial overlap → known IoU\n",
    "b1 = {'center': np.array([0, 0, 0]), 'dims': np.array([2, 2, 2])}\n",
    "b2 = {'center': np.array([1, 0, 0]), 'dims': np.array([2, 2, 2])}\n",
    "# Overlap: 1x2x2=4, union: 8+8-4=12, IoU=4/12=1/3\n",
    "assert abs(iou_3d_axis_aligned(b1, b2) - 1/3) < 1e-9, \"Sanity check failed\"\n",
    "\n",
    "print(\"IoU sanity checks passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mAP computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ap(gt_boxes, pred_boxes, iou_threshold=0.5):\n",
    "    \"\"\"Compute Average Precision for a single class (across all frames).\n",
    "    \n",
    "    Uses PASCAL VOC all-points interpolation.\n",
    "    Matching is constrained to boxes within the same frame (same frame_key).\n",
    "    \n",
    "    Args:\n",
    "        gt_boxes:   list of dicts with 'center', 'dims', 'frame_key'\n",
    "        pred_boxes: list of dicts with 'center', 'dims', 'confidence', 'frame_key'\n",
    "        iou_threshold: minimum IoU for a true positive (default 0.5)\n",
    "    \n",
    "    Returns:\n",
    "        ap:        Average Precision (float)\n",
    "        precision: array of precision values at each prediction\n",
    "        recall:    array of recall values at each prediction\n",
    "        tp_count:  total true positives\n",
    "        fp_count:  total false positives\n",
    "        fn_count:  total false negatives (unmatched GT boxes)\n",
    "    \"\"\"\n",
    "    # Edge cases\n",
    "    if len(gt_boxes) == 0 and len(pred_boxes) == 0:\n",
    "        return 1.0, np.array([]), np.array([]), 0, 0, 0\n",
    "    if len(gt_boxes) == 0:\n",
    "        return 0.0, np.zeros(len(pred_boxes)), np.zeros(len(pred_boxes)), 0, len(pred_boxes), 0\n",
    "    if len(pred_boxes) == 0:\n",
    "        return 0.0, np.array([]), np.array([]), 0, 0, len(gt_boxes)\n",
    "    \n",
    "    # Sort predictions by confidence (descending)\n",
    "    pred_sorted = sorted(pred_boxes, key=lambda x: x.get('confidence', 0.0), reverse=True)\n",
    "    \n",
    "    # Index GT boxes by frame for O(1) lookup\n",
    "    from collections import defaultdict\n",
    "    gt_by_frame = defaultdict(list)\n",
    "    for j, gt in enumerate(gt_boxes):\n",
    "        gt_by_frame[gt['frame_key']].append((j, gt))\n",
    "    \n",
    "    # Track which GT boxes have been matched (by global index)\n",
    "    gt_matched = set()\n",
    "    \n",
    "    tp = np.zeros(len(pred_sorted))\n",
    "    fp = np.zeros(len(pred_sorted))\n",
    "    \n",
    "    for i, pred in enumerate(pred_sorted):\n",
    "        best_iou = 0.0\n",
    "        best_gt_idx = -1\n",
    "        \n",
    "        # Only check GT boxes in the same frame\n",
    "        for j, gt in gt_by_frame.get(pred['frame_key'], []):\n",
    "            if j in gt_matched:\n",
    "                continue\n",
    "            iou = iou_3d_axis_aligned(pred, gt)\n",
    "            if iou > best_iou:\n",
    "                best_iou = iou\n",
    "                best_gt_idx = j\n",
    "        \n",
    "        if best_iou >= iou_threshold and best_gt_idx >= 0:\n",
    "            tp[i] = 1\n",
    "            gt_matched.add(best_gt_idx)\n",
    "        else:\n",
    "            fp[i] = 1\n",
    "    \n",
    "    # Cumulative sums\n",
    "    tp_cumsum = np.cumsum(tp)\n",
    "    fp_cumsum = np.cumsum(fp)\n",
    "    \n",
    "    precision = tp_cumsum / (tp_cumsum + fp_cumsum)\n",
    "    recall = tp_cumsum / len(gt_boxes)\n",
    "    \n",
    "    # AP using all-points interpolation (PASCAL VOC style)\n",
    "    # Add sentinel values at beginning and end\n",
    "    mrec = np.concatenate(([0.0], recall, [1.0]))\n",
    "    mpre = np.concatenate(([1.0], precision, [0.0]))\n",
    "    \n",
    "    # Make precision monotonically decreasing (right to left)\n",
    "    for k in range(len(mpre) - 2, -1, -1):\n",
    "        mpre[k] = max(mpre[k], mpre[k + 1])\n",
    "    \n",
    "    # Find points where recall changes\n",
    "    change_points = np.where(mrec[1:] != mrec[:-1])[0] + 1\n",
    "    \n",
    "    # Sum rectangular areas under the PR curve\n",
    "    ap = np.sum((mrec[change_points] - mrec[change_points - 1]) * mpre[change_points])\n",
    "    \n",
    "    tp_count = int(tp.sum())\n",
    "    fp_count = int(fp.sum())\n",
    "    fn_count = len(gt_boxes) - tp_count\n",
    "    \n",
    "    return float(ap), precision, recall, tp_count, fp_count, fn_count\n",
    "\n",
    "\n",
    "print(\"compute_ap() defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Parse GT boxes into list of dicts ─────────────────────────────────────────\n",
    "def parse_gt_boxes(df):\n",
    "    \"\"\"Parse GT CSV rows into box dicts.\n",
    "    GT columns: ego_x, ego_y, ego_z, ego_yaw, class_id (1-4),\n",
    "                center_x, center_y, center_z, width, length, height, yaw\n",
    "    \"\"\"\n",
    "    boxes = []\n",
    "    for _, row in df.iterrows():\n",
    "        frame_key = (int(row['ego_x']), int(row['ego_y']),\n",
    "                     int(row['ego_z']), int(row['ego_yaw']))\n",
    "        boxes.append({\n",
    "            'center': np.array([row['center_x'], row['center_y'], row['center_z']], dtype=np.float64),\n",
    "            'dims':   np.array([row['width'], row['length'], row['height']], dtype=np.float64),\n",
    "            'class_id': int(row['class_id']),\n",
    "            'frame_key': frame_key,\n",
    "        })\n",
    "    return boxes\n",
    "\n",
    "\n",
    "# ── Parse predicted boxes into list of dicts ──────────────────────────────────\n",
    "def parse_pred_boxes(df):\n",
    "    \"\"\"Parse prediction CSV rows into box dicts.\n",
    "    Pred columns (Airbus format): ego_x, ego_y, ego_z, ego_yaw,\n",
    "        bbox_center_x, bbox_center_y, bbox_center_z,\n",
    "        bbox_width, bbox_length, bbox_height, bbox_yaw,\n",
    "        class_ID (0-3), class_label\n",
    "    \n",
    "    Converts Airbus class_ID (0-3) to internal class_id (1-4) by adding 1.\n",
    "    \"\"\"\n",
    "    boxes = []\n",
    "    for _, row in df.iterrows():\n",
    "        frame_key = (int(row['ego_x']), int(row['ego_y']),\n",
    "                     int(row['ego_z']), int(row['ego_yaw']))\n",
    "        boxes.append({\n",
    "            'center': np.array([row['bbox_center_x'], row['bbox_center_y'], row['bbox_center_z']], dtype=np.float64),\n",
    "            'dims':   np.array([row['bbox_width'], row['bbox_length'], row['bbox_height']], dtype=np.float64),\n",
    "            'class_id': int(row['class_ID']) + 1,   # Airbus 0-3 → internal 1-4\n",
    "            'frame_key': frame_key,\n",
    "            'confidence': 1.0,  # No confidence score in CSV → uniform\n",
    "        })\n",
    "    return boxes\n",
    "\n",
    "\n",
    "# ── Parse boxes ───────────────────────────────────────────────────────────────\n",
    "gt_boxes_all  = parse_gt_boxes(df_gt_scene8)\n",
    "pred_boxes_all = parse_pred_boxes(df_pred)\n",
    "\n",
    "print(f\"GT boxes  (scene_8): {len(gt_boxes_all)}\")\n",
    "print(f\"Pred boxes (scene_8): {len(pred_boxes_all)}\")\n",
    "\n",
    "# ── Verify frame key overlap ──────────────────────────────────────────────────\n",
    "gt_frames  = set(b['frame_key'] for b in gt_boxes_all)\n",
    "pred_frames = set(b['frame_key'] for b in pred_boxes_all)\n",
    "common_frames = gt_frames & pred_frames\n",
    "gt_only = gt_frames - pred_frames\n",
    "pred_only = pred_frames - gt_frames\n",
    "\n",
    "print(f\"\\nFrame overlap:\")\n",
    "print(f\"  GT frames:     {len(gt_frames)}\")\n",
    "print(f\"  Pred frames:   {len(pred_frames)}\")\n",
    "print(f\"  Common frames: {len(common_frames)}\")\n",
    "print(f\"  GT-only frames (missed entirely): {len(gt_only)}\")\n",
    "print(f\"  Pred-only frames (hallucinated):  {len(pred_only)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Compute AP per class and mAP ─────────────────────────────────────────────\n",
    "results = {}\n",
    "\n",
    "for cid in [1, 2, 3, 4]:\n",
    "    gt_cls   = [b for b in gt_boxes_all if b['class_id'] == cid]\n",
    "    pred_cls = [b for b in pred_boxes_all if b['class_id'] == cid]\n",
    "    \n",
    "    ap, prec, rec, tp, fp, fn = compute_ap(gt_cls, pred_cls, IOU_THRESHOLD)\n",
    "    \n",
    "    results[cid] = {\n",
    "        'ap': ap,\n",
    "        'tp': tp,\n",
    "        'fp': fp,\n",
    "        'fn': fn,\n",
    "        'precision': tp / (tp + fp) if (tp + fp) > 0 else 0.0,\n",
    "        'recall':    tp / (tp + fn) if (tp + fn) > 0 else 0.0,\n",
    "        'n_gt':   len(gt_cls),\n",
    "        'n_pred': len(pred_cls),\n",
    "        'prec_curve': prec,\n",
    "        'rec_curve':  rec,\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{CLASS_NAMES[cid]}:\")\n",
    "    print(f\"  GT={len(gt_cls)}, Pred={len(pred_cls)}\")\n",
    "    print(f\"  TP={tp}, FP={fp}, FN={fn}\")\n",
    "    print(f\"  Precision={results[cid]['precision']:.3f}, Recall={results[cid]['recall']:.3f}\")\n",
    "    print(f\"  AP@0.5 = {ap:.4f}\")\n",
    "\n",
    "# ── mAP ───────────────────────────────────────────────────────────────────────\n",
    "map_score = np.mean([results[c]['ap'] for c in [1, 2, 3, 4]])\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"  mAP@IoU=0.5 = {map_score:.4f}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Pretty summary table ──────────────────────────────────────────────────────\n",
    "rows = []\n",
    "for cid in [1, 2, 3, 4]:\n",
    "    r = results[cid]\n",
    "    rows.append({\n",
    "        'Class': CLASS_NAMES[cid],\n",
    "        'GT': r['n_gt'],\n",
    "        'Pred': r['n_pred'],\n",
    "        'TP': r['tp'],\n",
    "        'FP': r['fp'],\n",
    "        'FN': r['fn'],\n",
    "        'Precision': f\"{r['precision']:.3f}\",\n",
    "        'Recall': f\"{r['recall']:.3f}\",\n",
    "        'AP@0.5': f\"{r['ap']:.4f}\",\n",
    "    })\n",
    "\n",
    "# Add total row\n",
    "total_gt   = sum(results[c]['n_gt'] for c in [1,2,3,4])\n",
    "total_pred = sum(results[c]['n_pred'] for c in [1,2,3,4])\n",
    "total_tp   = sum(results[c]['tp'] for c in [1,2,3,4])\n",
    "total_fp   = sum(results[c]['fp'] for c in [1,2,3,4])\n",
    "total_fn   = sum(results[c]['fn'] for c in [1,2,3,4])\n",
    "rows.append({\n",
    "    'Class': '--- TOTAL ---',\n",
    "    'GT': total_gt,\n",
    "    'Pred': total_pred,\n",
    "    'TP': total_tp,\n",
    "    'FP': total_fp,\n",
    "    'FN': total_fn,\n",
    "    'Precision': f\"{total_tp/(total_tp+total_fp):.3f}\" if (total_tp+total_fp) > 0 else \"N/A\",\n",
    "    'Recall': f\"{total_tp/(total_tp+total_fn):.3f}\" if (total_tp+total_fn) > 0 else \"N/A\",\n",
    "    'AP@0.5': f\"{map_score:.4f} (mAP)\",\n",
    "})\n",
    "\n",
    "df_results = pd.DataFrame(rows)\n",
    "print(df_results.to_string(index=False))\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"  mAP@IoU=0.5 = {map_score:.4f}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "print(f\"\\nNote: IoU is axis-aligned (ignoring yaw rotation).\")\n",
    "print(f\"This is a conservative lower bound — oriented IoU would likely give slightly higher scores.\")\n",
    "print(f\"Predictions have uniform confidence=1.0 (no ranking), so AP = precision at max recall.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "\n",
    "## Interpreting the results\n",
    "\n",
    "- **AP@0.5 per class**: how well the model detects each obstacle type. Values range from 0 (no correct detections) to 1 (perfect detection at all recall levels).\n",
    "- **mAP@0.5**: the mean across all 4 classes — this is the **primary Airbus metric**.\n",
    "- **Precision**: fraction of predictions that are correct (low = too many false positives / hallucinations).\n",
    "- **Recall**: fraction of GT boxes that are detected (low = too many misses).\n",
    "- **FN (false negatives)**: obstacles we completely missed — dangerous in a collision avoidance system.\n",
    "\n",
    "## Key factors affecting scores\n",
    "\n",
    "1. **No confidence scores**: predictions all have confidence=1.0, so AP cannot benefit from ranking good predictions higher. If the model outputs confidence, AP could improve significantly.\n",
    "2. **Axis-aligned IoU**: ignoring yaw means elongated objects (cables, poles) may have artificially lower IoU when rotated. Oriented IoU would likely improve scores.\n",
    "3. **Cable difficulty**: cables are thin (median height 0.16m) with few LiDAR points — hardest class to detect.\n",
    "4. **Frame key matching**: ego pose values are cast to int for matching. If there is floating-point drift between GT and pred CSVs, some frames may not match (check `GT-only` and `Pred-only` counts above).\n",
    "\n",
    "## What to improve\n",
    "\n",
    "- Add **confidence scores** to predictions (softmax probability, cluster density, etc.)\n",
    "- Implement **oriented 3D IoU** for more accurate evaluation\n",
    "- Focus training on **recall for cables** (most safety-critical misses)\n",
    "- Increase **NMS threshold tuning** to reduce duplicate detections (if FP is high)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}